{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**@author: James V. Talwar**<br>\n",
    "\n",
    "# Generating VADEr Feature Patches:\n",
    "\n",
    "**About:** This notebook generates <font color='red'>**VADEr**</font> (SNP) feature patches as a f(x) of a user-defined patch window length (W)/patch size (`patch_window_length`), which is equivalent to the desired divided by 2. \n",
    "\n",
    "For example, below in this notebook, `patch_window_length = 2.5e5` (250kb), but if other patch window lengths are desired, the user should feel free to update this parameter below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import joblib\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "console = logging.StreamHandler()\n",
    "logger.addHandler(console)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USER: Update the following paths/parameters as required for your purposes:**\n",
    " - `patch_window_length`: Desired patch window size for VADEr features\n",
    " - `feature_directory`: Path to all candidate feature sets\n",
    "    - **NB:** Here, the assumption is that all candidate feature sets have the following naming convention -> dataset.p-value.extract.txt (as one might use for formulating extracts with PLINK(2). If you want to just run this notebook without any issues, ensure you follow this convention. If you prefer a different structure, you will need to adapt the `snpSets = ...` below.\n",
    " - `write_path`: Path/directory to which want to write all needed patch formulation files. It is expected that `write_path` will have two directories within it: `Patches_To_Features` and `Patch_To_Chrom_Mapping`, each of which will have a folder `patch_window_length`kb (e.g. `250kb`) to which files will be written. To modify this behavior, adapt the final cell of this notebook to which file writes are directed.\n",
    " - `chromosome_length_file`: Path to file with mapping of each chromosome to chromosome length. Hg19/GRCh37 chromosome sizes can be found at our [DARTH_VADEr repository](https://github.com/jvtalwar/DARTH_VADEr/tree/main) at `VADEr/Data/Reference_Genome_Build_Sizes/hg19.chrom.sizes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_window_length = 2.5e5 \n",
    "feature_directory = \"../../Data/snps/extract/PC\" \n",
    "write_path = \"../../Data/Feature_Patches\"\n",
    "chromosome_length_file = \"../../Data/Reference_Genome_Build_Sizes/hg19.chrom.sizes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Patch Spacing \n",
    " - Obtaining midpoints of each patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_radius = patch_window_length/2 #generate a patch_radius from patch window (similar to Plink's --clump-kb parameter in clumping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chr1': 249250621,\n",
       " 'chr2': 243199373,\n",
       " 'chr3': 198022430,\n",
       " 'chr4': 191154276,\n",
       " 'chr5': 180915260,\n",
       " 'chr6': 171115067,\n",
       " 'chr7': 159138663,\n",
       " 'chr8': 146364022,\n",
       " 'chr9': 141213431,\n",
       " 'chr10': 135534747,\n",
       " 'chr11': 135006516,\n",
       " 'chr12': 133851895,\n",
       " 'chr13': 115169878,\n",
       " 'chr14': 107349540,\n",
       " 'chr15': 102531392,\n",
       " 'chr16': 90354753,\n",
       " 'chr17': 81195210,\n",
       " 'chr18': 78077248,\n",
       " 'chr19': 59128983,\n",
       " 'chr20': 63025520,\n",
       " 'chr21': 48129895,\n",
       " 'chr22': 51304566,\n",
       " 'chrX': 155270560,\n",
       " 'chrY': 59373566}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in chromosome lengths/sizes\n",
    "chromosome_lengths = pd.read_csv(chromosome_length_file, sep = \"\\t\", header = None)\n",
    "chromosome_lengths.columns = [\"CHR\", \"LENGTH\"]\n",
    "iteratables = [\"chr\" + str(i) for i in range(1,23)] + [\"chrX\", \"chrY\"]\n",
    "chromLengthMap = {k:v for k,v in dict(zip(chromosome_lengths.CHR, chromosome_lengths.LENGTH)).items() if k in iteratables}\n",
    "chromLengthMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromValidPartitions = defaultdict(list)\n",
    "for chrom, length in chromLengthMap.items():\n",
    "    numChromRegions = length//(patch_radius * 2)\n",
    "    i = 0\n",
    "    midpoint = patch_radius\n",
    "    while midpoint <= length:\n",
    "        chromValidPartitions[chrom].append(midpoint)\n",
    "        i += 1\n",
    "        midpoint = i*2*patch_radius + patch_radius\n",
    "    \n",
    "    # A final valid partition can be excluded when the distance between the chr length and the final given partition is < 2 * plink window. \n",
    "    # In this case need to add on a final partition == chr length which encapsulates length - plink window  \n",
    "    if (length - ((i-1)*2*patch_radius + patch_radius)) > patch_radius: \n",
    "        chromValidPartitions[chrom].append(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chr1 right distance encapsulated by final partition: 0\n",
      "chr1 left distance encapsulated by final partition: 621.0\n",
      "\n",
      "chr2 right distance encapsulated by final partition: 74373.0\n",
      "chr2 left distance encapsulated by final partition: 125000.0\n",
      "\n",
      "chr3 right distance encapsulated by final partition: 0\n",
      "chr3 left distance encapsulated by final partition: 22430.0\n",
      "\n",
      "chr4 right distance encapsulated by final partition: 29276.0\n",
      "chr4 left distance encapsulated by final partition: 125000.0\n",
      "\n",
      "chr5 right distance encapsulated by final partition: 40260.0\n",
      "chr5 left distance encapsulated by final partition: 125000.0\n",
      "\n",
      "chr6 right distance encapsulated by final partition: 0\n",
      "chr6 left distance encapsulated by final partition: 115067.0\n",
      "\n",
      "chr7 right distance encapsulated by final partition: 13663.0\n",
      "chr7 left distance encapsulated by final partition: 125000.0\n",
      "\n",
      "chr8 right distance encapsulated by final partition: 0\n",
      "chr8 left distance encapsulated by final partition: 114022.0\n",
      "\n",
      "chr9 right distance encapsulated by final partition: 88431.0\n",
      "chr9 left distance encapsulated by final partition: 125000.0\n",
      "\n",
      "chr10 right distance encapsulated by final partition: 0\n",
      "chr10 left distance encapsulated by final partition: 34747.0\n",
      "\n",
      "chr11 right distance encapsulated by final partition: 0\n",
      "chr11 left distance encapsulated by final partition: 6516.0\n",
      "\n",
      "chr12 right distance encapsulated by final partition: 0\n",
      "chr12 left distance encapsulated by final partition: 101895.0\n",
      "\n",
      "chr13 right distance encapsulated by final partition: 44878.0\n",
      "chr13 left distance encapsulated by final partition: 125000.0\n",
      "\n",
      "chr14 right distance encapsulated by final partition: 0\n",
      "chr14 left distance encapsulated by final partition: 99540.0\n",
      "\n",
      "chr15 right distance encapsulated by final partition: 0\n",
      "chr15 left distance encapsulated by final partition: 31392.0\n",
      "\n",
      "chr16 right distance encapsulated by final partition: 0\n",
      "chr16 left distance encapsulated by final partition: 104753.0\n",
      "\n",
      "chr17 right distance encapsulated by final partition: 70210.0\n",
      "chr17 left distance encapsulated by final partition: 125000.0\n",
      "\n",
      "chr18 right distance encapsulated by final partition: 0\n",
      "chr18 left distance encapsulated by final partition: 77248.0\n",
      "\n",
      "chr19 right distance encapsulated by final partition: 3983.0\n",
      "chr19 left distance encapsulated by final partition: 125000.0\n",
      "\n",
      "chr20 right distance encapsulated by final partition: 0\n",
      "chr20 left distance encapsulated by final partition: 25520.0\n",
      "\n",
      "chr21 right distance encapsulated by final partition: 4895.0\n",
      "chr21 left distance encapsulated by final partition: 125000.0\n",
      "\n",
      "chr22 right distance encapsulated by final partition: 0\n",
      "chr22 left distance encapsulated by final partition: 54566.0\n",
      "\n",
      "chrX right distance encapsulated by final partition: 0\n",
      "chrX left distance encapsulated by final partition: 20560.0\n",
      "\n",
      "chrY right distance encapsulated by final partition: 0\n",
      "chrY left distance encapsulated by final partition: 123566.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validate (i.e., ensure no issues with final patch assignment for each chromosome):\n",
    "for c,l in chromLengthMap.items():\n",
    "    right_distance = l - max(chromValidPartitions[c])\n",
    "    if right_distance < 0 or right_distance > patch_radius:\n",
    "        logging.warning(\"WARNING: Discrepancy issue at chrom {}: dist between end chrom and final window center point {}\".format(c, right_distance))\n",
    "        \n",
    "    else:\n",
    "        logging.info(\"{} right distance encapsulated by final partition: {}\".format(c, right_distance))\n",
    "        logging.info(\"{} left distance encapsulated by final partition: {}\\n\".format(c, max(chromValidPartitions[c]) - (chromValidPartitions[c][-2] + patch_radius)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify all candidate feature sets under investigation and extract/format needed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "snpSets = {file.split(\".\")[1]:pd.read_csv(os.path.join(feature_directory, file), header = None, dtype = str)[0].tolist() for file in os.listdir(feature_directory) if \"extract\" in file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting and processing SNPs at p-value threshold of 5e-08\n",
      "\n",
      "Extracting and processing SNPs at p-value threshold of 5e-07\n",
      "\n",
      "Extracting and processing SNPs at p-value threshold of 5e-05\n",
      "\n",
      "Extracting and processing SNPs at p-value threshold of 5e-06\n",
      "\n",
      "Extracting and processing SNPs at p-value threshold of 5e-04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gottaCatchEmAllPlinkymon = defaultdict(list)\n",
    "for k,v in snpSets.items():\n",
    "    logging.info(\"Extracting and processing SNPs at p-value threshold of {}\\n\".format(k))\n",
    "    for snp in v:\n",
    "        assert \"rs\" not in snp\n",
    "        chrom, basePair, ref, alt = snp.split(\":\")\n",
    "        gottaCatchEmAllPlinkymon[k].append([chrom, snp, basePair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHR</th>\n",
       "      <th>SNP</th>\n",
       "      <th>BP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52704</th>\n",
       "      <td>1</td>\n",
       "      <td>1:10008013:G:T</td>\n",
       "      <td>10008013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52705</th>\n",
       "      <td>1</td>\n",
       "      <td>1:10008365:G:T</td>\n",
       "      <td>10008365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52706</th>\n",
       "      <td>1</td>\n",
       "      <td>1:10008526:T:C</td>\n",
       "      <td>10008526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52756</th>\n",
       "      <td>1</td>\n",
       "      <td>1:10012311:G:A</td>\n",
       "      <td>10012311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52757</th>\n",
       "      <td>1</td>\n",
       "      <td>1:10014173:C:G</td>\n",
       "      <td>10014173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CHR             SNP        BP\n",
       "52704   1  1:10008013:G:T  10008013\n",
       "52705   1  1:10008365:G:T  10008365\n",
       "52706   1  1:10008526:T:C  10008526\n",
       "52756   1  1:10012311:G:A  10012311\n",
       "52757   1  1:10014173:C:G  10014173"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert dictionary to dataframe: \n",
    "pokedex = {k:pd.DataFrame(v, columns = [\"CHR\", \"SNP\", \"BP\"]).sort_values(by = [\"CHR\", \"BP\"]) for k,v in gottaCatchEmAllPlinkymon.items()}\n",
    "pokedex[\"5e-04\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate/Sanity Check: Ensure no SNP BP exceeds the chromosome length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, df in pokedex.items():\n",
    "    chromosomes = sorted(list(set(df.CHR)))\n",
    "    for charmander in chromosomes:\n",
    "        v = df[df.CHR == charmander] #subset to chromosome\n",
    "        basePairs = list(v.BP.astype(int))\n",
    "        if charmander == 23:\n",
    "            charmander = \"X\"\n",
    "        if max(basePairs) > chromLengthMap[\"chr\" + str(charmander)]:\n",
    "            logging.warning(\"WARNING {} CHR {}: Highest SNP BP exceeds the valid chromosome range...\".format(k, charmander))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add column for each SNP set DF pertaining to its chromosomal patch (as defined by the `patch_radius`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input(s): \n",
    "1) chromPartitions - A dictionary of centered valid chromosomal partitions with a radius of patch_radius \n",
    "2) clumpSummaryDF - A dataframe of sorted (by CHR and BP) SNPs for a particular feature set \n",
    "\n",
    "Output(s):\n",
    "1) A dataframe with a chromosomal level index mappings according to the allotted patch_radius - these are/will be/can be used for adding positional information to InSNPtion after clump compression\n",
    "'''\n",
    "def AssignSNPPatchIndex(chromPartitions, clumpSummaryDF):\n",
    "    radius = chromPartitions[\"chr1\"][0]\n",
    "    logging.info(\"Radius of chromosomal partitions is {}\".format(radius))\n",
    "    chromosomes = sorted(list(set(clumpSummaryDF.CHR)))\n",
    "    thatMakesYouChartMan = list() #positional mapping\n",
    "    for i, row in clumpSummaryDF.iterrows():\n",
    "        charmander = \"chr\" + str(row[\"CHR\"])\n",
    "        if charmander == \"chr23\":\n",
    "            charmander = \"chrX\"\n",
    "            \n",
    "        clumpChromPosition =  np.abs(np.asarray(chromPartitions[charmander]) - int(row[\"BP\"])).argmin()\n",
    "        thatMakesYouChartMan.append(clumpChromPosition)\n",
    "        \n",
    "    clumpSummaryDF[\"ChromosomePatch\"] = thatMakesYouChartMan\n",
    "    \n",
    "    clumpSummaryDF.CHR = clumpSummaryDF.CHR.apply(lambda x: \"23\" if x == \"X\" else x)\n",
    "    clumpSummaryDF.CHR = clumpSummaryDF.CHR.astype(int)\n",
    "    clumpSummaryDF.BP = clumpSummaryDF.BP.astype(int)\n",
    "    \n",
    "    return clumpSummaryDF.sort_values(by = [\"ChromosomePatch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SNP set 5e-08\n",
      "Radius of chromosomal partitions is 125000.0\n",
      "Processing SNP set 5e-07\n",
      "Radius of chromosomal partitions is 125000.0\n",
      "Processing SNP set 5e-05\n",
      "Radius of chromosomal partitions is 125000.0\n",
      "Processing SNP set 5e-06\n",
      "Radius of chromosomal partitions is 125000.0\n",
      "Processing SNP set 5e-04\n",
      "Radius of chromosomal partitions is 125000.0\n"
     ]
    }
   ],
   "source": [
    "patch_mappings = dict()\n",
    "for k,v in pokedex.items():\n",
    "    logging.info(\"Processing SNP set {}\".format(k))\n",
    "    patch_mappings[k] = AssignSNPPatchIndex(chromPartitions = chromValidPartitions, clumpSummaryDF = v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and write VADEr patch mappings (for all candidate feature sets): **1) Patch feature dictionaries** and **2) Patch to chromosome locations** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dictionaries for SNP set 5e-08\n",
      "Generating dictionaries for SNP set 5e-07\n",
      "Generating dictionaries for SNP set 5e-05\n",
      "Generating dictionaries for SNP set 5e-06\n",
      "Generating dictionaries for SNP set 5e-04\n"
     ]
    }
   ],
   "source": [
    "for k,v in patch_mappings.items():\n",
    "    logging.info(\"Generating dictionaries for SNP set {}\".format(k))\n",
    "    pikaGroup = v.groupby([\"CHR\", \"ChromosomePatch\"])\n",
    "    features = defaultdict(list)\n",
    "    positionalInformation = defaultdict(lambda: defaultdict(int)) #convert to DF before saving - joblib hates lambda f(x)s\n",
    "    clumpNum = 1\n",
    "    chromInOrderCheck = list()\n",
    "    patchInOrderCheck = list()\n",
    "    prevKey = -1\n",
    "    for keysGroupedBy, dataGroupedBy in pikaGroup:\n",
    "        features[\"clump\" + str(clumpNum)] = list(dataGroupedBy.sort_values(by = [\"BP\"]).SNP)\n",
    "        positionalInformation[\"clump\" + str(clumpNum)][\"CHR\"] = keysGroupedBy[0] \n",
    "        positionalInformation[\"clump\" + str(clumpNum)][\"ChromosomePatch\"] = keysGroupedBy[1] \n",
    "        \n",
    "        chromInOrderCheck.append(keysGroupedBy[0])\n",
    "        \n",
    "        if prevKey != keysGroupedBy[0]:\n",
    "            prevKey = keysGroupedBy[0]\n",
    "            if (len(patchInOrderCheck) == 0):\n",
    "                patchInOrderCheck.append(keysGroupedBy[1])\n",
    "            else:\n",
    "                if (patchInOrderCheck != sorted(patchInOrderCheck)):\n",
    "                    logging.warning(\"Chromosomal patches may be out of order\")\n",
    "                patchInOrderCheck = [keysGroupedBy[1]]\n",
    "        else:\n",
    "            patchInOrderCheck.append(keysGroupedBy[1])\n",
    "        \n",
    "        clumpNum += 1\n",
    "    \n",
    "    #type check\n",
    "    chromsAsArray = np.array(chromInOrderCheck)\n",
    "    isAnInt = np.issubdtype(chromsAsArray.dtype, np.integer)\n",
    "    isAFloat = np.issubdtype(chromsAsArray.dtype, np.floating)\n",
    "    \n",
    "    assert isAnInt or isAFloat, f\"Chromosomes not stored as int/floats and violate numerical ordering. Current dtype {np.array(chromInOrderCheck).dtype}\"\n",
    "    if (chromInOrderCheck != sorted(chromInOrderCheck)): \n",
    "        logging.warning(\"Patches may be out of order\")\n",
    "        \n",
    "    #convert positional info into DF\n",
    "    positionalInformation = pd.DataFrame(positionalInformation).T\n",
    "    assert (positionalInformation.CHR == sorted(positionalInformation.CHR)).all(), \"Chromosomes to patch ordering is askew...\"\n",
    "    \n",
    "    #Save relevant structures:\n",
    "    joblib.dump(features, os.path.join(write_path, \"Patches_To_Features/{}kb\".format(str(int(2*patch_radius//1e3))), k + \"_{}kb_DistanceClumps.joblib\".format(int(2*patch_radius/1000))))\n",
    "    positionalInformation.to_csv(os.path.join(write_path, \"Patch_To_Chrom_Mapping/{}kb\".format(str(int(2*patch_radius//1e3))), k + \"_{}kb_PositionalMaps.tsv\".format(int(2*patch_radius/1000))), sep = \"\\t\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intersect2.0",
   "language": "python",
   "name": "intersect2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
