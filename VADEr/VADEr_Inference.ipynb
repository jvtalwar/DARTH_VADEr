{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**@author: James V. Talwar**\n",
    "\n",
    "# VADEr Inference: Computing Polygenic Risk Scores with VADEr\n",
    "\n",
    "**About:** This notebook provides a detailed walkthrough of how to generate polygenic risk scores (PRSs) with VADEr, and can be used either in standalone format (i.e., as is) or as a template/tutorial for generating a VADEr inference script. In particular, this notebook covers the following:\n",
    " - VADEr model instantiation\n",
    " - Loading the optimal (i.e., best validation set performance) trained-checkpointed model\n",
    " - Generating PRSs with VADEr for a dataset of interest\n",
    " - Calculating performance metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys \n",
    "sys.path.append('./src/') #path to VADEr, SNP_Dataset, utility functions etc...\n",
    "\n",
    "from VADErData import SNP_Dataset\n",
    "from VADErDataUtils import GenerateChromosomePatchMask\n",
    "from vader import VADEr\n",
    "from MetricUtils import BinaryClassAccuracy, MultiClassAccuracy, Calc_ROC_AUC \n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "console = logging.StreamHandler()\n",
    "logger.addHandler(console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun  7 17:59:25 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A30                     On  |   00000000:81:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             29W /  165W |       4MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "About: Load configuration from yaml-config path\n",
    "\n",
    "Input(s): path: String corresponding to path to yaml config file for training.\n",
    "'''\n",
    "def LoadConfig(path):\n",
    "    return yaml.load(open(path, 'r'), Loader = yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USER: Update the following paths/parameters below:**\n",
    " - `config_path`: Path to config employed for VADEr training.\n",
    " - `prediction_write_path`: Path to (existing) directory to which want to write dataset VADEr predictions/PRSs.\n",
    " - number_workers: The total number of workers to use for dataloading. Update this according to your needs/resources.\n",
    " - `prediction_file_name`: (Optional) File name (not including extension) for VADEr prediction/PRSs. By default this file name will match your config `training_summary_path`.\n",
    " - `metric`: (Optional) String corresponding to the metric for which want to load the best performing k-checkpointed VADEr model. By default `metric` will match your config specified `checkpoint_metric`, which ensures the best performing model by your defined metric is loaded (as this was the metric by which k-checkpointing was conducted). Valid options: `{Val_Disease_Accuracy, Val_AUC, Val_Loss, Train_Disease_Accuracy, Train_AUC, Train_Loss}`.\n",
    " - `feather_path`: File path to [composite-level](https://github.com/jvtalwar/DARTH_VADEr/wiki/Enabling-Dataloading:-Data-Processing-and-Expected-Formatting#which-should-i-choose---composite-level-vs-individual-level-feathers-important-considerations) test set genotype feather file.\n",
    "   - Writing/reading individual-level feathers can also be employed here, you will just need to define a dataset (e.g., test set) specific `cached_feather_path` below and pass that to `cache_write_path` in SNP_Dataset object initialization (variable name `dataset` below).\n",
    " - `pheno_path`: File path to all dataset (e.g., test set) [phenotypes](https://github.com/jvtalwar/DARTH_VADEr/wiki/Enabling-Dataloading:-Data-Processing-and-Expected-Formatting#formatting-phenotypes).\n",
    " - `test_ids_path`: File path to all dataset (e.g., test set) IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = #<-- UPDATE WITH RELEVANT PATH \n",
    "prediction_write_path = \"../Predictions\" #<-- UPDATE WITH RELEVANT PATH (ensure directory exists) \n",
    "number_workers = 16 #<-- UPDATE AS NEEDED ACCORDING TO AVAILABLE RESOURCES\n",
    "\n",
    "config = LoadConfig(config_path)\n",
    "\n",
    "dataset_params = config[\"dataset\"]\n",
    "model_params = config[\"model_params\"]\n",
    "train_and_checkpoint_params = config[\"train_and_checkpoint\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_file_name = os.path.basename(train_and_checkpoint_params[\"training_summary_path\"])\n",
    "metric = train_and_checkpoint_params[\"checkpoint_metric\"]\n",
    "\n",
    "valid_metric = {\"Val_Disease_Accuracy\", \"Val_AUC\", \"Val_Loss\", \"Train_Disease_Accuracy\", \"Train_AUC\", \"Train_Loss\"}\n",
    "\n",
    "assert metric in valid_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define all dataset (e.g., test set) paths:\n",
    "feather_path = #<-- UPDATE WITH RELEVANT PATH \n",
    "pheno_path = #<-- UPDATE WITH RELEVANT PATH \n",
    "test_ids_path = #<-- UPDATE WITH RELEVANT PATH \n",
    "\n",
    "#cached_feather_path = ... <-- Define desired write directory if during PRS generation want to write/read from individual-level feathers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the best performing VADEr model by defined `metric`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trained VADEr model by defined metric Val_AUC occurs at epoch 32 with the following training/validation set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_Disease_Accuracy</th>\n",
       "      <th>Train_Ancestry_Accuracy</th>\n",
       "      <th>Train_AUC</th>\n",
       "      <th>Train_Loss</th>\n",
       "      <th>Val_Disease_Accuracy</th>\n",
       "      <th>Val_Ancestry_Accuracy</th>\n",
       "      <th>Val_AUC</th>\n",
       "      <th>Val_Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.7077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765795</td>\n",
       "      <td>0.563552</td>\n",
       "      <td>0.680015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74864</td>\n",
       "      <td>0.625232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Train_Disease_Accuracy  Train_Ancestry_Accuracy  Train_AUC  Train_Loss  \\\n",
       "32                  0.7077                      NaN   0.765795    0.563552   \n",
       "\n",
       "    Val_Disease_Accuracy  Val_Ancestry_Accuracy  Val_AUC  Val_Loss  \n",
       "32              0.680015                    NaN  0.74864  0.625232  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_alignment = metric == train_and_checkpoint_params[\"checkpoint_metric\"]\n",
    "\n",
    "performance_summary = pd.read_csv(train_and_checkpoint_params[\"training_summary_path\"], sep = \"\\t\", index_col = 0)\n",
    "\n",
    "if not metric_alignment:\n",
    "    k_checkpointed_models = [int(model.split(\".\")[0].split(\"_\")[-1]) for model in os.listdir(train_and_checkpoint_params[\"model_checkpoint_path\"])]\n",
    "    performance_summary = performance_summary.loc[k_checkpointed_models, :]\n",
    "    \n",
    "if metric in {\"Val_Loss\", \"Train_Loss\"}: #minimization metric\n",
    "    best_performing_model = performance_summary[metric].idxmin()\n",
    "    \n",
    "else:\n",
    "    best_performing_model = performance_summary[metric].idxmax()\n",
    "    \n",
    "logger.info(f\"Best trained VADEr model by defined metric {metric} occurs at epoch {best_performing_model} with the following training/validation set performance:\")\n",
    "pd.DataFrame(performance_summary.loc[best_performing_model, :]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize SNP_Dataset object and dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid age path given: /cellar/users/jtalwar/projects/BetterRiskScores/InSNPtion/Galbatorix/DucksInARow/TrainingSetStatistics/ELLIPSE/TrainingSetAgeStats.pkl\n",
      "   Returning z-scored ages in loader...\n",
      "1607 SNPs exist across the full 5e-4 dataset with INF or NULL values. Removing these now... Unremoved SNP set size is 110292\n",
      "Cleaned SNP set size after removal of invalid SNPs is 108685\n",
      "Filtering SNP set for MAF and genotype consistent SNPs...\n",
      "Cleaned SNP set size after filtering incompatible genotype and MAF discrepancy SNPs is 12017\n",
      "100%|██████████| 1204/1204 [00:00<00:00, 3603.55it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = SNP_Dataset(featherFilePath = feather_path,\n",
    "                      phenoFilePath = pheno_path,\n",
    "                      idFilePath = test_ids_path, \n",
    "                      snpSubsetPath = dataset_params.get(\"SNP_set\"),\n",
    "                      validMafSnpsPath = dataset_params.get(\"consistent_maf_SNPs\"),\n",
    "                      vaderPatchMappingPath = dataset_params.get(\"patch_mapping_path\"),\n",
    "                      trainingSetAgeStatsPath = dataset_params.get(\"age_train_stats\"), \n",
    "                      sparsePatchThreshold = dataset_params.get(\"sparse_patch_threshold\"),\n",
    "                      enableShifting = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset = dataset, \n",
    "                        pin_memory = True, \n",
    "                        shuffle = False, \n",
    "                        batch_size = train_and_checkpoint_params[\"batch_size\"] * torch.cuda.device_count() * 2, \n",
    "                        num_workers = number_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate and load best trained VADEr model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Implementing LEARNABLE CLS token representation.\n",
      "Enabling 8 registers\n",
      "Implementing SwiGLU... correcting mlpDim to 2048 to keep number params consistent\n",
      "Implementing learnable temperature for MHA: True\n",
      "Implementing learnable temperature for MHA: True\n",
      "Implementing learnable temperature for MHA: True\n",
      "Implementing learnable temperature for MHA: True\n",
      "Implementing learnable temperature for MHA: True\n",
      "Implementing learnable temperature for MHA: True\n",
      "Implementing learnable temperature for MHA: True\n",
      "Implementing learnable temperature for MHA: True\n",
      "Implementing learnable temperature for MHA: True\n",
      "Implementing learnable temperature for MHA: True\n",
      "Implementing learnable temperature for MHA: True\n",
      "Implementing learnable temperature for MHA: True\n"
     ]
    }
   ],
   "source": [
    "masking = train_and_checkpoint_params.get(\"masking\") \n",
    "mask = None\n",
    "if masking == \"chrom\":\n",
    "    mask = GenerateChromosomePatchMask(patch_to_chrom_mapping_path = dataset_params.get(\"patch_to_chrom_mapping\"),\n",
    "                                       feature_patches = dataset.patchSizes)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "attention = model_params.get(\"attention\")\n",
    "patch_layer_norm = model_params.get(\"patch_layer_norm\")\n",
    "num_registers = model_params.get(\"num_registers\")\n",
    "\n",
    "if attention is None:\n",
    "    attention = \"MHA\"\n",
    "\n",
    "if patch_layer_norm is None:\n",
    "    patch_layer_norm = attention == \"LSA\"\n",
    "\n",
    "if num_registers is None:\n",
    "    num_registers = 0\n",
    "\n",
    "vader_model = VADEr(patchSizes = dataset.patchSizes,\n",
    "                   modelDim = model_params[\"model_dim\"],\n",
    "                   mlpDim = model_params[\"model_dim\"] * model_params[\"mlp_scale\"],\n",
    "                   depth = model_params[\"num_transformer_blocks\"],\n",
    "                   attnHeads = model_params[\"num_attention_heads\"],\n",
    "                   attnHeadDim = model_params[\"model_dim\"]//model_params[\"num_attention_heads\"],\n",
    "                   multitaskOutputs = model_params[\"prediction_dims\"],\n",
    "                   clumpProjectionDropout = model_params[\"patch_projection_dropout\"],\n",
    "                   dropout = model_params[\"model_dropout\"], \n",
    "                   ageInclusion = model_params[\"age_inclusion\"],\n",
    "                   aggr = model_params[\"aggregation\"],\n",
    "                   context = model_params.get(\"cls_representation\"),\n",
    "                   patchProjectionActivation = model_params[\"non_linear_patch_projection\"],\n",
    "                   patchLayerNorm = patch_layer_norm,\n",
    "                   trainingObjective = \"cross_entropy\",\n",
    "                   attention = attention,\n",
    "                   numRegisters = num_registers,\n",
    "                   ffActivation = model_params.get(\"mlp_method\"),\n",
    "                   contrastive_projection_net_dim = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(train_and_checkpoint_params[\"model_checkpoint_path\"], f\"VADEr_Epoch_{best_performing_model}.pt\")\n",
    "vader_model.load_state_dict(torch.load(model_path)[\"modelStateDict\"], strict = False)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    torch.nn.DataParallel(vader_model, list(range(torch.cuda.device_count())))\n",
    "    \n",
    "vader_model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define method for evaluation and prediction file generation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method to evaluate a trained VADEr model and write predictions. Returns a dictionary of metrics.\n",
    "'''\n",
    "@torch.no_grad()\n",
    "def Eval_Model(model, loader, device, IDs, write_path, file_name, mask):\n",
    "    model.eval()\n",
    "    \n",
    "    raw = torch.Tensor()\n",
    "    labels = torch.Tensor()\n",
    "    metrics = defaultdict(float)\n",
    "    lossFx = nn.BCEWithLogitsLoss(reduction = \"sum\")\n",
    "    \n",
    "    for i, (patchBatch, diseaseStatusBatch, ancestryBatch, fHBatch, zAgeBatch) in enumerate(loader):\n",
    "        gpuClumpBatch = {k:v.to(device) for k,v in patchBatch.items()} #features\n",
    "        diseaseStatusBatch = diseaseStatusBatch.to(device) #labels\n",
    "\n",
    "        if model.includeAge: #including age - need to pass in more than clump dictionary\n",
    "            output = model(dictOfClumps = gpuClumpBatch, mask = mask, age_batch = zAgeBatch.to(device))\n",
    "        else:\n",
    "            output = model(dictOfClumps = gpuClumpBatch, mask = mask)\n",
    "        \n",
    "        metrics[\"Loss\"] += lossFx(output[\"disease\"], diseaseStatusBatch.float()) #<-- if model is sup_con pretrained, will need to change output[\"disease\"] to output - can check if self.projectionnetwork in model? --> getattr(model, \"projectionNetwork\")\n",
    "        \n",
    "        raw = torch.cat([raw, output[\"disease\"].to(\"cpu\")], dim = 0)\n",
    "        labels = torch.cat([labels, diseaseStatusBatch.to(\"cpu\")], dim = 0)\n",
    "        \n",
    "\n",
    "    predictions = torch.sigmoid(raw)\n",
    "    \n",
    "    #Return metrics:\n",
    "    metrics[\"Loss\"] = metrics[\"Loss\"].item()/len(loader.dataset)\n",
    "    metrics[\"Disease_Accuracy\"] = BinaryClassAccuracy(preds = raw, labels = labels)\n",
    "    \n",
    "    metrics[\"AUC\"] = Calc_ROC_AUC(preds = predictions, labels = labels)\n",
    "    \n",
    "    logger.info(f\"Loss {metrics['Loss']:.5f}\")\n",
    "    logger.info(f\"ACCURACY {metrics['Disease_Accuracy']:.5f}\")\n",
    "    logger.info(f\"AUC {metrics['AUC']:.5f}\")\n",
    "    \n",
    "    #Write predictions:\n",
    "    predictionFile = pd.DataFrame([[el[0].item() for el in predictions], [el[0].item() for el in labels]], index = [\"Predictions\", \"Labels\"]).T\n",
    "    predictionFile.index = IDs \n",
    "    predictionFile.to_csv(os.path.join(write_path, file_name), sep = \"\\t\")\n",
    "    \n",
    "    logger.info(f\"VADEr predictions written to: {os.path.join(write_path, file_name)}\")\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate predictions and obtain dataset level performance across metrics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.65670\n",
      "ACCURACY 0.66400\n",
      "AUC 0.72665\n",
      "VADEr predictions written to: ../Predictions/67_VADEr_bs4096_lr8e-05_sc-cosine_with_warmup_wu95_clip10_dim768_mlp4_bl12_he12_pd0.2_md0.2_ag-cls_cls-learnable_act-True_at-MHA-LT_nr-8_ffAct-SwiGLU_mask-None_spt-False_pln-False_wd0.1_b2-0.99_nm-z_clumped.tsv\n"
     ]
    }
   ],
   "source": [
    "dataset_metrics = Eval_Model(model = vader_model, \n",
    "                            loader = dataloader, \n",
    "                            device = device, \n",
    "                            IDs = dataloader.dataset.datasetIDs,\n",
    "                            write_path = prediction_write_path,\n",
    "                            file_name = prediction_file_name,\n",
    "                            mask = mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intersect2.0",
   "language": "python",
   "name": "intersect2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
